%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
Machine learning models are trained with large amounts of individual data and automated decisions are made on these predictive models. Privacy and fairness arise as important issues that we must consider to protect people from possibly adverse effects of such systems. We must consider the privacy implications of collecting and using personal information together with the biases embedded in datasets and algorithms, and the consequences of the resulting classifications and segmentation \cite{Dwork-l:2013}.
Ideally, privacy and fairness should be considered by design, but before we must solve some key questions, such as how privacy protection technologies may improve or impede the fairness of the systems they affect, and many other questions in the research agenda suggested by \cite{Ekstrand:2018}. 

In this paper we show that privacy protection mechanisms and fairness, may be improved together, having positive interactions between them. 
We devise an algorithm based on the well-known algorithm MDAV for providing $k$-anonymity by microaggregation with fairness guarantees to answer affirmatively the following two questions for this privacy protection method, as stated in \cite{Ekstrand:2018}.
 
\begin{itemize}
\item Does the system provide comparable privacy protections to different groups of subjects?
\item Can privacy protection technologies or policies be used or adapted to enhance the fairness of a system? 
\end{itemize} 


\subsection{Roadmap}

This paper is organized as follows. In Section \ref{sec:related}, we review the related work and the state of the art on integrated privacy and fairness, then we present definitions and related work on each separately.
In Section \ref{sec:our-approach}, we define our algorithm to provide fair privacy. 
Our experimental framework is provided in Section \ref{sec:experiments}.
Finally, we discuss the conclusions of this work and future work in Section \ref{sec:conclusions}.
