%%%%%%%%%%%
% SECTION %
%%%%%%%%%%%
\section{Experimental Framework}
\label{sec:experiments}

In this section, we describe the experimental framework used to analyse and compare the information-loss induced and the data-utility retained after performing Fair-MDAV. 

\subsection{Data}
\label{subsec:datasets}

Our experiments were performed on the \emph{Adult Dataset}~\cite{Dua2019}; this dataset was chosen since it is commonly used as benchmark in both privacy and fairness literature. It consists of 14 categorical and integer attributes, with 48842 instances. The usual classification task for this dataset is to predict its \texttt{label}: whether an individual earns more than $\$50,000$ per year. For our experiments, we consider the the \texttt{sex} attribute to be the PA.

Adult is PA-imbalanced, with one third of the instances being \emph{female} and two thirds being \emph{male}. It also presents PA-bias with respect to the \texttt{label}, since only $10\%$ of the females are labelled as positive, while the positive proportion for males is $30\%$.

Prior to performing our experiments, the following preprocessing was applied to data:
\begin{itemize}
  \item An irrelevant attribute (\texttt{fnlwgt}) was dropped.
  \item Numerical attributes (\texttt{age}, \texttt{education-num}, \texttt{capital-gain}, \texttt{capital-loss} and \texttt{hours-per-week}) were binned into five categories each.
  \item All variables were one-hot encoded for better classifier compatibility.
\end{itemize}

\subsection{Experiments}
\label{subsec:experiments}

There are two important parameters in Fair-MDAV:
\begin{enumerate}
  \item The size of each fairlet, represented by $k$.
  \item The amount of fairness correction to be introduced, represented by $\tau$.
\end{enumerate} 
For each of these, we performed an experiment, fixing the remaining parameters. In both cases, we set the PA unfavoured/favoured proportion in fairlets to 3/7, since this is very close to the dataset's PA proportion. The dataset was train/test split to an 80/20 proportion. Only the train sets were transformed using Fair-MDAV, and logistic regression classifiers were learnt from the transformed train sets. The following datasets were generated:
\begin{itemize}
  \item 10 train sets with $\tau = 1$ and $k = \{10, 20, \dots, 100\}$.
  \item 11 train sets with $k = 10$ and $\tau = \{0, 0.1, \dots, 1.0\}$.
\end{itemize}

In order to measure the information loss associated to increasing $k$ on the first experiment, we evaluated the average distance of datapoints to their fairlet representative. The resulting mean square errors (MSEs) may be observed on Figure~\ref{fig:MSE}. As expected, larger $k$-values produced a higher MSE.

Figure~\ref{fig:performance} shows the resulting performance in predicting both the test and the original train sets. Even though predicting the train set's label is unusual, we believe that in our setting the performance differences between predicting the train and the test sets are an indicator of how well data is anonymised: better performance over the train set would indicate information leakage into the model, while similar performance is indicative of an adequate generalisation. Interestingly, in our experiments the learnt classifiers performed consistently better over the test set than over the train set.

Classifiers learnt from the transformed sets never outperformed the classifier learnt from the unmodified train set (baseline in Figure~\ref{fig:performance}), as is to be expected. However, the loss in accuracy was always around $2\%$, with AUC and F1 scores very close to the baseline at parameter values $k = 10, \tau = 1$. As a general trend, we may say that increasing the $\tau$ value seems to improve both AUC and F1, while larger $k$ values tend to underperform (as this implies larger groups for privacy protection).

Regarding fairness, we considered metrics associated to demographic parity and equality of opportunity. As may be seen in Figure~\ref{fig:fairness}, classification fairness drops substantially if no fairness-correction is performed, e.g. when $\tau = 0$. However, large $\tau$ values, e.g. $\tau \approx 1$ more than make up for this and yield classifiers outperforming the baseline and in the case of EOR getting close to an optimal EOR of 1. Increasing the value of $k$ also seems to have a negative impact over the resulting classifier's fairness, yet fixing $\tau = 1$ consistently produced fairer classifiers than the baseline one. In a trend similar to the one followed by performance metrics, fairness performance on train set predictions was consistently similar to performance on test predictions.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{../Plots/k_MSE.pdf}
  \caption{MSE across $k$-values for $k$ in $\{10, 20, \dots, 100\}$.}
  \label{fig:MSE}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{../Plots/performance_facet.pdf}
  \caption{Accuracy, AUC and F1 scores for different $\tau$ and $k$ values, for \emph{test} (as a measure of performance) and \emph{train} (as a potential measure of privacy).}
  \label{fig:performance}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{../Plots/fairness_facet.pdf}
  \caption{DPR and EOR for different $\tau$ and $k$ values, for \emph{test} (as a measure of performance) and \emph{train} (as a potential measure of privacy).}
  \label{fig:fairness}
\end{figure}
