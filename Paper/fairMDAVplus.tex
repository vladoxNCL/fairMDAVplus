\RequirePackage{amsmath}
\documentclass[runningheads]{llncs}

\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage[table, dvipsnames]{xcolor}

%\usepackage{todonotes}
\usepackage{soul}
%
%\newcommand{\julNote}[1]{\todo[color=blue!40,noinline,fancyline, size=\tiny]{Jul: #1}}
%\newcommand{\julInline}[1]{\todo[color=blue!40,inline,fancyline, size=\tiny]{Jul: #1}}
%
%\newcommand{\vladNote}[1]{\todo[color=green!40,noinline,fancyline, size=\tiny]{Vlad: #1}}
%\newcommand{\vladInline}[1]{\todo[color=green!40,inline,fancyline, size=\tiny]{Vlad: #1}}


\DeclareMathOperator*{\argmin}{arg\,min}

% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\unskip#1{<#1>}%
}

\begin{document}
%
\title{Fair-MDAV: An Algorithm for Fair Privacy \\ by Microaggregation \thanks{This work was supported by the Spanish Government, in part under Grant RTI2018-095094-B-C22 ``CONSENT''.}}
%
%\titlerunning{FairMDAV}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Juli치n Salas~\inst{1,2} \and Vladimiro Gonz치lez-Zelaya~\inst{3,4}}

\authorrunning{J. Salas and V. Gonz치lez}

\institute{
Internet Interdisciplinary Institute (IN3) \\
Universitat Oberta de Catalunya \\
\and
Center for Cybersecurity Research of Catalonia \\
Barcelona, Spain \\ 
\email{jsalaspi@uoc.edu}
\and
Newcastle University, Newcastle upon Tyne, UK
\and
Universidad Panamericana, Mexico City, Mexico\\
Escuela de Ciencias Econ칩micas y Empresariales \\
\email{cvgonzalez@up.edu.mx}
}

\maketitle

\begin{abstract}
Automated decision systems are being integrated to several institutions. The General Data Protection Regulation from the European Union, considers the right to explanation on such decisions, but many systems may require a group-level or community-wide analysis.
However, the data on which the algorithms are trained is frequently personal data. Hence, the privacy of individuals should be protected, at the same time, ensuring the fairness of the algorithmic decisions made.
In this paper we present the algorithm Fair-MDAV for privacy protection in terms of $t$-closeness.
We show that its microaggregation procedure for privacy protection improves fairness through relabelling, while the improvement on fairness obtained equalises privacy guarantees for different groups.
We perform an empirical test on Adult Dataset, carrying out the classification task of predicting whether an individual earns $\$50,000$ per year, after applying Fair-MDAV with different parameters on the training set. 
We observe that the accuracy of the results on the test set is well preserved, with additional guarantees of privacy and fairness.

\keywords{fair classification \and $t$-closeness \and fair privacy}
\end{abstract}
	
% SECTIONS

%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
Machine learning models are trained with large amounts of individual data and automated decisions are made on these predictive models. Privacy and fairness arise as important issues that we must consider to protect people from possibly adverse effects of such systems. We must consider the privacy implications of collecting and using personal information together with the biases embedded in datasets and algorithms, and the consequences of the resulting classifications and segmentation~\cite{Dwork-l:2013}.
Privacy and fairness should be considered by design, as they are key principles of the General Data Protection Regulation (GDPR) from EU. Hence, we must solve some relevant questions, such as how privacy protection technologies may improve or impede the fairness of the systems they affect, and many other questions in the research agenda suggested by~\cite{Ekstrand:2018}. 

In this paper we show that privacy protection and fairness improvement mechanisms can be applied at the same time, having positive interactions between them. 
We devise an algorithm based on the well-known MDAV~\cite{Domingo:2005} algorithm for providing $k$-anonymity by microaggregation with fairness guarantees, to answer affirmatively the following two questions for this privacy protection method, as stated in~\cite{Ekstrand:2018}.
\begin{itemize}
\item Does the system provide comparable privacy protections to different groups of subjects?
\item Can privacy protection technologies or policies be used or adapted to enhance the fairness of a system? 
\end{itemize} 

From the fair classification point of view, our method provides a natural connection between two families of fairness definitions, namely those based on groups and those based on individuals. 

\subsection{Roadmap}

This paper is organised as follows. In Section \ref{sec:related}, we review the related work and the state of the art on integrated privacy and fairness, then we present definitions and related work on each separately.
In Section \ref{sec:our-approach}, we define our algorithm to provide fair privacy. 
Our experimental framework is provided in Section \ref{sec:experiments}.
Finally, we discuss the conclusions of this work and future work in Section \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work on Integrated Privacy and Fairness}
\label{sec:related}
There is plenty of work on privacy literature and on fairness but few studies relate both. In this section we present recent works that carry out research on the intersection of both, then we present some related work and definitions for fairness and for privacy.
In~\cite{Hajian:2015} $k$-anonymity was used to protect frequent patterns with fairness.
While~\cite{Dwork:2012} discussed when fairness in classification implies privacy, and how differential privacy may be related to fairness. New models of differential private and fair logistic regression are provided in~\cite{Xu:2019}. A study on how differential privacy may have disparate impact is performed in~\cite{Pujol:2020}, they carry out an experiment with the two real-world decisions made using U.S. Census data of allocating federal funds and assignment of voting rights benefits. 
The position paper~\cite{Ekstrand:2018} argues in general for integrating research on fairness and non-discrimination to socio-technical systems that provide privacy protection.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fairness Related Work and Definitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%
Automated classifiers decide on a variety of tasks ranging from whether bank loans are approved to deciding on the early release of prisoners based on their likelihood to recidivate. These decisions may be made by classifiers learnt from biased data, causing unfair decisions to be made and even causing these biases to be perpetuated.
%
%\subsubsection{Motivation}
%
Many different definitions of fairness have been proposed~\cite{kusner2017counterfactual}, which are sometimes in contrast with one another. A decision rule satisfying one of the definitions may well prove to be very unfair for a different one~\cite{chouldechova2017fair}. For example, determining university admissions through gender quotas may achieve similar rates of acceptance across this variable, but make the acceptance rates for good students of different genders disparate. We suggest that a classifier's unfair behaviour may be corrected through relabelling for the unfavoured group to match the favoured group's positive ratio at the fairlet level. According to~\cite{berk2018fairness}, one of the main problems with data relabelling is the loss of prediction accuracy caused by such interventions. In section~\ref{sec:experiments}, we show that even with the most aggressive correction level, the accuracy-loss due to our method is relatively low. Our method was inspired by fair clustering~\cite{Chierichetti:2017,backurs2019scalable}: we evaluate a dataset's \emph{fairlets}---subsets of datapoints located close to each other with respect to a distance function, with a demographic distribution similar to the overall dataset's---which in the clustering context help in obtaining \emph{fair} clusters. We use fairlets with two distinct purposes: to anonymise data and to enhance fairness on classification tasks meant to be learned over the original data. Fair classification requires defining several concepts in order to properly measure a classifier's level of fairness. These definitions, necessary to understand \emph{group fairness}---the family of fairness metrics we have analysed---follow.

\begin{definition}[Positive and Negative Label]
  A binary classifier's labels may usually take \emph{positive} or \emph{negative} values, referring to how desirable an outcome in predictions may be. These outcomes could be, for instance, whether an application for college admission is successful or not. In this case, the positive label would refer to getting accepted, and the negative one to being rejected.
\end{definition}

\begin{definition}[Protected Attribute]
  A \emph{protected attribute} \textup{(}PA\textup{)} of a dataset, refers to a feature prone to discrimination, due to many possible factors. In our case we will be dealing with a single binary PA, meaning there will only be two PA groups, with every datapoint belonging to one of these groups.
\end{definition}

\begin{definition}[Positive Ratio]
  We will call the ratio of the number of positive instances divided by the total number of instances in a specific group the \emph{positive ratio} \textup{(}PR\textup{)} of the group. 
\end{definition}

\begin{definition}[Favoured and Unfavoured Groups]
  Among the two PA groups, the one having the highest PR will be referred to as the \emph{favoured} \textup{(}$F$\textup{)} group, while the other one will be referred to as the \emph{unfavoured} \textup{(}$U$\textup{)} group. So we will denote by FPR the favoured positive ratio and by UPR the unfavoured positive ratio.
\end{definition}

\subsubsection{Fairness Metrics}

There are many different fairness definitions~\cite{kusner2017counterfactual,kilbertus2017avoiding}, on this paper we will focus on two of them.

\begin{definition}[Demographic Parity]
  A classifier satisfies \emph{demographic parity} if the probability of being classified as positive is the same across PA subgroups:
  \begin{equation}
    \label{eq:dp}
    \text{P} \left( \hat{Y} = 1 \mid \text{PA} = U \right) =
    \text{P} \left( \hat{Y} = 1 \mid \text{PA} = F \right).
  \end{equation}
\end{definition}

\begin{definition}[Equality of Opportunity]
  A classifier satisfies \emph{equality of opportunity} if the probability of being classified as positive \emph{for true positives} is the same across PA subgroups:
  \begin{equation}
    \label{eq:eo}
    \text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = U \right) =
    \text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = F \right).
  \end{equation}
\end{definition}

Demographic parity and equality of opportunity may be expressed as the following ratios, respectively:
\[
  \text{DPR} \coloneqq \frac
  {\text{P} \left( \hat{Y} = 1 \mid \text{PA} = U \right)}
  {\text{P} \left( \hat{Y} = 1 \mid \text{PA} = F \right)}, \quad
  \text{EOR} \coloneqq \frac
  {\text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = U \right)}
  {\text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = F \right)}.
\]

These ratios will measure how fair a classifier's predictions are with respect to a fairness definition: the closer the ratio is to 1, the fairer a classifier will be.

\subsubsection{Correcting for Fair Classification}

Fairness-aware machine learning is defined by~\cite{friedler2019comparative} as a set of preprocessing techniques that modify input data so that any classifier trained on such data will be fair. According to~\cite{kamiran2012data}, there are four main ways in which to make appropriate adjustments to data to enforce fairness: suppressing certain features, also known as \emph{fairness through unawareness}~\cite{gajane2017formalizing},  reweighing features~\cite{krasanakis2018adaptive}, resampling data instances~\cite{rubin1973use,kamiran2010classification,salimi2019capuchin,zelaya2019parametrised} and \emph{massaging} variable values~\cite{chiappa2018path}; our method belongs to this last category.

\subsection{Privacy Related Work and Definitions}

There is a vast and growing literature on privacy protection techniques. 
Regarding data publishing we may consider that the two main privacy models are $k$-anonymity (and its enhancements) and differential privacy. There are some differences and interactions between them in different contexts, some of them are discussed in~\cite{Salas:2018}. We will focus on $t$-closeness, which is an improvement of $k$-anonymity.

\cite{Samarati:2001,Sweeney:2002} showed that removing all the \emph{identifiers} (IDs), such as social security number or name-surname, does not prevent the individual's re-identification. Unique combinations of attribute values, called \emph{quasi-identifiers} (QIs) may be used instead of the IDs for re-identification. After this, individuals' \emph{sensitive attributes} (SAs) are revealed, e.g. salary, medical conditions, etc. To prevent re-identification, they defined $k$-anonymity as follows.

\begin{definition}[$k$-Anonymity]
A dataset is \emph{$k$-anonymous} if each record is indistinguishable from at least other $k - 1$ records within the dataset, when considering the values of its QIs. 
\end{definition}

There are several techniques for obtaining $k$-anonymous datasets, e.g. generalisation, suppression and microaggregation.

\begin{definition}[Microaggregation]
To obtain microaggregates in dataset with $n$ records, these are combined to form groups $g$ of size at least $k$ ($k$-groups). For each attribute, the average value over each group is computed and is used to replace each of the original averaged values. Groups are formed using a criterion of maximal similarity.
\end{definition}

The optimal $k$-partition (from the information loss point of view) is defined to be the one that maximises within-group homogeneity. The higher the within-group homogeneity, the lower the information loss, since microaggregation replaces values in a group by the group centroid. The sum of squares criterion is common to measure homogeneity in clustering. The within-groups sum of squares SSE is defined as:
$$SSE= \displaystyle\sum_{j=1}^{n_g}\sum_{i=1}^{n_j}d(x_{ij},\overline{x}_j)^2$$
Where $x_{ij}$ denotes the $i-$th record in the $j-$th group, $\overline{x}_j$ is the average record in the group $j$, $n_g$ is the number of groups and $n_j$ is the number of elements in the $j-$th group.

MDAV (Maximum Distance to AVerage)~\cite{Domingo:2005} is an algorithm for $k$-anonymisation based on microaggregation it has been used extensively and modified in several different ways. Two recent examples are~\cite{Salas:2018-b}, in which it is used for $k$-anonymisation of dynamic data and other is~\cite{Rodriguez:2020}, in which MDAV's efficiency is largely improved without losing precision. By design, $k$-anonymity guarantees that a user's record is protected from linkage with probability higher than $1/k$. However, if the SAs are not modified, a low variability of SAs in a group will help an adversary to infer the SAs of a user, even without being able to link her. 
Hence, some modifications to $k$-anonymity have been done, such as $\ell$-diversity~\cite{Machanavajjhala:2006} and $t$-closeness~\cite{Li:2007}. 

\begin{definition}[$t$-Closeness]
An equivalence class satisfies $t$-closeness if the distance between the distribution of the SAs of the individuals in the group to the distribution of the SAs in the whole table is not greater than a threshold $t$.
A dataset $D$ (usually a $k$-anonymous dataset) satisfies \emph{$t$-closeness} if all its equivalence classes ($k$-groups) satisfy $t$-closeness.
\end{definition}

\section{Fair-MDAV} 
\label{sec:our-approach}
In this section, we present the Fair-MDAV algorithm, designed to improve fairness while giving some guarantees of user's privacy. From now on, we consider that the PA and the label in sense of fairness are also SAs privacy-wise, i.e. neither the PA nor the label should be inferred easily from the QIs. As a concrete example, in a gender study of income, neither the gender nor the income of an individual should be inferred from the data, to prevent from discrimination and from invasion of privacy. On each iteration Fair-MDAV finds the element that is furthest from the average record, generates a fairlet, removes these elements, and iterates. After obtaining all the fairlets, calculates the center of the fairlet (average record) and replaces all the original records by their corresponding center (algorithm \ref{alg:microaggregation}). Finally, Fair-MDAV relabels the labels of the records in the unfavored class in each fairlet while the positive ratio for them is less than $\tau$ times the positive ratio for the favored class, see algorithm \ref{alg:tau}. 

The main properties of Fair-MDAV are that it is able to generate fairlets for fair clustering. It provides $t-$closeness regarding the PA by setting the proportion $n_U/n_F$ to approximate the proportion $|U|/|F|$ in the full set, in the more restrictive case, setting $n_U=|U|$ and $n_F = |F|$. Fair-MDAV also provides $(n_U + n_F)$-anonymity by design and it equalises the privacy guarantees for both classes of PA with respect to the SA label, this is obtained by improving fairness with the \texttt{Tau} function, as the $\textit{UPR}$ is increased by relabelling while it is less than $\tau * \textit{FPR}$.

\begin{algorithm}[p]
\SetKwProg{Fn}{Function}{}{end}
\SetKwFunction{Fairlets}{fairlets}
\SetKwFunction{Centers}{centers}
\SetKwFunction{FairMDAV}{Fair-MDAV}
\Fn{\Fairlets{$df, n_U, n_F$}}{
\KwIn{dataframe, size of $U$, size of $F$}
\While{$|\{x\in df :label(x) = U\}| \geq n_U
\text{ and } |\{x\in df :label(x) = F\}| \geq n_F $}
{
  $x_r = argmax\{d(x,\overline{x})\}$
  where $\overline{x} = avg\{x\in df\}$
	
  $U = \{x_U\}$ the $n_U \text{ closest records to } x_r
  \text{ such that } labels(x_U) = U$

  $F = \{x_F\}$ the $n_F \text{ closest records to } x_r
  \text{ such that } labels(x_F) = F$

  $g = U\cup F$ (must have $n_U + n_F$ records including $x_r$)

  Remove $g$ from $df$
        
  Add $g$ to $\mathcal{G}$ list of fairlets.    
}
\KwRet $\mathcal{G}$ list of fairlets
}
\Fn{\Centers{$df$, $\mathcal{G}$}}{
\For{$g\in  \mathcal{G}$}
{
  $\overline{x} = avg\{x \in g\}$
  $x = \overline{x}$ for $x\in g\subset df$
  (replace record by group centroid)
}
\KwRet $\overline{df}$
}
\caption{Microaggregation operations of Fair-MDAV}
\label{alg:microaggregation}
\end{algorithm}

\begin{algorithm}[p]
\SetKwProg{Fn}{Function}{}{end}
\SetKwFunction{FTau}{Tau}

\Fn{\FTau{$df, {\mathcal G}, \tau$}}{
\KwIn{dataframe, clusters, parameter}
\For{$C\in  \mathcal{G}$}{
  $\textit{FPR}(C) = \left| \frac{\{x \in C \mid \textit{pa}(x)=F \text{ and } \textit{label}(x)=1\}}{\left\{ x \in C \mid \textit{pa}(x) = F \right\}} \right|$

  $\textit{UPR}(C) = \left| \frac{\{x \in C \mid \textit{pa}(x)=U \text{ and } \textit{label}(x)=1\}}{\left\{ x \in C \mid \textit{pa}(x) = U \right\}} \right|$

  $\textit{U}_0 = \{x \in C \mid \textit{pa}(x)=U \text{ and } \textit{label}(x)=0\}$

  \While{$\textit{UPR}(C) < \tau * \textit{FPR}(C)$ and $|\textit{U}_0| > 0$}
        {$\textit{label}(x)=1$ for some $x\in \textit{U}_0$} 
}
\KwRet $\widetilde{df}$
}
\caption{Tau function for relabelling}
\label{alg:tau}
\end{algorithm}

\begin{algorithm}[p]
\SetKwProg{Fn}{Function}{}{end}
%\SetKwFunction{Fairlets}{fairlets}
%\SetKwFunction{Centers}{centers}
\SetKwFunction{FairMDAV}{Fair-MDAV}
\SetKwFunction{FTau}{tau}

\Fn{\FairMDAV{$df, n_U, n_F, \tau$}}{
\KwIn{dataframe, size of $U$, size of $F$, parameter}
$\mathcal{G} = \Fairlets(df, n_U, n_F)$

$\overline{df}= \Centers (\mathcal{G}, df)$

$\widetilde{df}= \FTau(\overline{df},  \mathcal{G}, \tau)$	

\KwOut{$\widetilde{df}$}
}
\caption{Pseudo code of Fair-MDAV}
\label{alg:Fair-MDAV}
\end{algorithm}

\subsection{Toy Example}

Table~\ref{tab:toy} illustrates the full process over a simple dataset, consisting of five individuals with one unprotected and one protected attributes, as well as a binary label. The following list summarises the process:

\begin{itemize}
  \item Algorithm 1 (Fairlets and Centers):
    \begin{enumerate}
      \item Assume $n_U = n_F = 1$ and $\tau = 1$.
      \item Find $c = \textit{mean}(\text{Feat})$: $c = 0.424$.
      \item Use farthest point from $c$ w.r.t. Feat to initialise $C_1$ (id = $\{4\}$).
      \item Add to $C_1$ the closest points such that $n_U$ and $n_F$ hold (id = $\{4, 1 \}$).
      \item Repeat and find $C_2 = \{2, 5 \}$.
      \item Since no more clusters are possible, discard $\{3\}$.
      \item Replace Feat with corresponding centroid values.
    \end{enumerate}  \item Algorithm 2 (Fairness):
    \begin{enumerate}
      \item Since $\textit{FPR}(C_1) = 1$ and $\textit{UPR}(C_1) = 0$, relabel $\{4\}$.
      \item Since $\textit{FPR}(C_2) = 0$ and $\textit{UPR}(C_2) = 0$, no relabelling is performed.
    \end{enumerate}
\end{itemize}

\begin{table}
  \caption{\footnotesize Toy example. During the first phase of the method, two clusters are detected, imputing the cluster's centroid values of the unprotected attributes (\emph{Feat}) into each cluster's members; the resulting clusters have been coloured green ($C_1$) and red ($C_2$), while the blue datapoint is discarded. On the second phase, fairness is corrected by making $\textit{UPR} \ge \textit{FPR}$ ($\tau = 1$) for each cluster.}
  \centering
  \begin{tabular}{ccccccccccccc}
    && \multicolumn{3}{c}{Original Data} && \multicolumn{3}{c}{Microaggregation} && \multicolumn{3}{c}{Fairness Correction} \\
    \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-13}
    \enskip id \enskip & $\quad$ & \enskip Feat \enskip & \enskip PA \enskip & \enskip Label \enskip && \enskip Feat \enskip & \enskip PA \enskip & \enskip Label \enskip && \enskip Feat \enskip & \enskip PA \enskip & \enskip Label \enskip \\
    \rowcolor{ForestGreen!30} 1 & \cellcolor{white} & 0.95 & F & 1 & \cellcolor{white} & 0.99 & F & 1 & \cellcolor{white} & 0.99 & F & 1\\
    \rowcolor{Lavender!30} 2 & \cellcolor{white} & 0.05 & F & 0 &  \cellcolor{white} & 0.04 & F & 0 & \cellcolor{white} & 0.04 & F & 0 \\
    \rowcolor{SkyBlue!30} 3 & \cellcolor{white} & 0.06 & F & 1 & \cellcolor{white} \multirow{-2}{*}{\Large $\enskip \rightarrow \enskip$} & \multicolumn{3}{c}{\emph{Dropped}} & \cellcolor{white} \multirow{-2}{*}{\Large $\enskip \rightarrow \enskip$} & \multicolumn{3}{c}{\emph{Dropped}} \\
    \rowcolor{ForestGreen!30} 4 & \cellcolor{white} & 1.03 & U & 0 & \cellcolor{white} & 0.99 & U & 0 & \cellcolor{white} & 0.99 & U & 1 \\
    \rowcolor{Lavender!30} 5 & \cellcolor{white} & 0.03 & U & 0 & \cellcolor{white} & 0.04 & U & 0 & \cellcolor{white} & 0.04 & U & 0 \\
  \end{tabular}
  \label{tab:toy}
\end{table}

%%%%%%%%%%%
% SECTION %
%%%%%%%%%%%
\section{Experimental Framework}
\label{sec:experiments}

In this section, we describe the experimental framework used to analyse and compare the information-loss induced and the data-utility retained after performing Fair-MDAV. 

\subsection{Data}
\label{subsec:datasets}
Our experiments were performed on the \emph{Adult Dataset}~\cite{Dua2019}; this dataset was chosen since it is commonly used as benchmark in both privacy and fairness literature. It consists of 14 categorical and integer attributes, with 48842 instances. The usual classification task for this dataset is to predict its \texttt{label}: whether an individual earns more than $\$50,000$ per year. For our experiments, we consider the the \texttt{sex} attribute to be the PA.
Adult is PA-imbalanced, with one third of the instances being \emph{female} and two thirds being \emph{male}. It also presents PA-bias with respect to the \texttt{label}, since only $10\%$ of the females are labelled as positive, while the positive proportion for males is $30\%$. Prior to our experiments, the following preprocessing was applied to data:
\begin{itemize}
  \item The attribute \texttt{fnlwgt} was dropped, since it is a demographics parameter not usually considered for class prediction.
  \item Numerical attributes (\texttt{age}, \texttt{education-num}, \texttt{capital-gain}, \texttt{capital-loss} and \texttt{hours-per-week}) were binned into five categories each.
  \item All variables were one-hot encoded for better classifier compatibility.
\end{itemize}

\subsection{Experiments}
\label{subsec:experiments}

There are two important parameters in Fair-MDAV: The size of each fairlet, represented by $k$ and the amount of fairness correction to be introduced, represented by $\tau$. For each of these, we performed an experiment, fixing the remaining parameters.

In both cases, we set the PA unfavoured/favoured proportion in fairlets to 3/7, since this is very close to the dataset's PA proportion. The dataset was train/test split to an 80/20 proportion. Only the train sets were transformed using Fair-MDAV, and logistic regression classifiers were learnt from the transformed train sets. 
We generated 10 train sets with $\tau = 1$ and $k = \{10, 20, \dots, 100\}$ and 11 train sets with $k = 10$ and $\tau = \{0, 0.1, \dots, 1.0\}$.

Figure~\ref{fig:performance} shows the resulting performance in predicting both the test and the original train sets. Even though predicting the train set's label is unusual, we believe that in our setting the performance differences between predicting the train and the test sets are an indicator of how well data is anonymised: better performance over the train set would indicate information leakage into the model, while similar performance is indicative of an adequate generalisation. Interestingly, in our experiments the learnt classifiers performed consistently better over the test set than over the train set.

Classifiers learnt from the transformed sets never outperformed the classifier learnt from the unmodified train set (baseline in figure~\ref{fig:performance}), as is to be expected. However, the loss in accuracy was always around $2\%$, with AUC and F1 scores very close to the baseline at parameter values $k = 10, \tau = 1$. As a general trend, we may say that increasing the $\tau$ value seems to improve both AUC and F1, while larger $k$ values tend to underperform (as this implies larger groups for privacy protection).

Regarding fairness, we considered metrics associated to demographic parity and equality of opportunity. As may be seen in figure~\ref{fig:fairness}, classification fairness drops substantially if no fairness-correction is performed, e.g. when $\tau = 0$. However, large $\tau$ values, e.g. $\tau \approx 1$ more than make up for this and yield classifiers outperforming the baseline and in the case of EOR getting close to an optimal EOR of 1. Increasing the value of $k$ also seems to have a negative impact over the resulting classifier's fairness, yet fixing $\tau = 1$ consistently produced fairer classifiers than the baseline one. In a trend similar to the one followed by performance metrics, fairness performance on train set predictions was consistently similar to performance on test predictions.

Finally, to measure the information loss associated to increasing $k$ on the first experiment, we evaluated the average distance of datapoints to their fairlet representative. The resulting mean square errors (MSEs) may be observed on figure~\ref{fig:MSE}. As expected, larger $k$-values produced a higher MSE.

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{../Plots/performance_facet.pdf}
  \caption{Accuracy, AUC and F1 scores for different $\tau$ and $k$ values, for \emph{test} (as a measure of performance) and \emph{train} (as a potential measure of privacy).}
  \label{fig:performance}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{../Plots/fairness_facet.pdf}
  \caption{DPR and EOR for different $\tau$ and $k$ values, for \emph{test} (as a measure of performance) and \emph{train} (as a potential measure of privacy).}
  \label{fig:fairness}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[height=0.4\textwidth]{../Plots/k_MSE.pdf}
  \caption{MSE across $k$-values for $k$ in $\{10, 20, \dots, 100\}$.}
  \label{fig:MSE}
\end{figure}

%%%%%%%%%%%
\section{Conclusions and Future Work}
\label{sec:conclusions}
%%%%%%%%%%%
In this paper, we have defined the algorithm Fair-MDAV for fair privacy. 
It provides $t-$closeness by considering the SAs when performing microaggregation. It integrates a method of relabelling on the microaggregates that improves fairness controlled by a parameter $\tau$.
We conducted an empirical evaluation on Adult dataset from US census, and demonstrated that our algorithm is able to reduce information loss, while simultaneously retain data utility and improving fairness in terms of demographic parity and equality of opportunity.

Our microaggregation method generates fairlets that may be used for fair clustering, nonetheless, we leave as future work to compare the quality of the fairlets generated by Fair-MDAV to state-of-the-art fair clustering algorithms.
We considered fairness measures of demographic parity and equality of opportunity, still, we would like to extend our results to other measures and apply them on more datasets including synthetically generated data.

\newpage
\bibliographystyle{splncs04}
\bibliography{biblio-fairnonymity}

\end{document}
