%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work on Integrated Privacy and Fairness}
\label{sec:related}
There is plenty of work on privacy literature and on fairness but few studies relate both. In this section we present recent works that carry out research on the intersection of both, then we present some related work and definitions for Fairness and for Privacy.
In \cite{Hajian:2015} $k$-anonymity was used to protect frequent patterns with fairness.
While \cite{Dwork:2012} discussed when fairness in classification implies privacy, and how differential privacy may be related to fairness. New models of differential private and fair logistic regression are provided in \cite{Xu:2019}. A study on how differential privacy may have disparate impact is performed in \cite{Pujol:2020}, they carry out an experiment with the two real-world decisions made using U.S. Census data of allocating federal funds and assignment of voting rights benefits. 
The position paper \cite{Ekstrand:2018} argues in general for integrating research on fairness and non-discrimination to socio-technical systems that provide privacy protection.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fairness Related Work and Definitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Motivation}

Many different definitions of fairness have been proposed~\cite{kusner2017counterfactual}, which are however sometimes in contrast with one another. A decision rule satisfying one of the definitions may well prove to be very unfair for a different one~\cite{chouldechova2017fair}. For example, determining university admissions through gender quotas may achieve similar rates of acceptance across this variable, but make the acceptance rates for good students of different genders disparate.

We suggest that a classifier's unfair behaviour may be corrected through label-swapping for the unfavoured group to match the favoured group's positive ratio at the fairlet level. 

According to~\cite{berk2018fairness}, one of the main problems with data relabelling is the loss of prediction accuracy caused by such interventions. In section~\ref{sec:experiments}, we show that even with the most aggressive correction level, the accuracy-loss due to our method is relatively low.

\subsubsection{Fair Clustering}

Our method was inspired by fair clustering~\cite{Chierichetti:2017,backurs2019scalable}: we evaluate a dataset's \emph{fairlets}---subsets of datapoints located close to each other with respect to a distance function, with a demographic distribution similar to the overall dataset's---which in the clustering context help in obtaining \emph{fair} clusters. We, however, use fairlets with two distinct purposes: to anonymise data and to enhance fairness on classification tasks meant to be learned over the original data. 

\subsubsection{Fair Classification}

In contrast to fair clustering, fair classification requires defining a number of concepts in order to properly measure a classifier's level of fairness. These definitions, necessary to understand \emph{group fairness}---the family of fairness metrics we have analysed---follow.

\begin{definition}[Positive and Negative Label]
  A binary classifier's labels may usually take \emph{positive} or \emph{negative} values, referring to how desirable an outcome in predictions may be. These outcomes could be, for instance, whether an application for college admission is successful or not. In this case, the positive label would refer to getting accepted, and the negative one to being rejected.
\end{definition}

\begin{definition}[Protected Attribute]
  A \emph{protected (or sensitive) attribute} (PA) of a dataset, refers to a feature prone to discrimination, due to many possible factors. In our case we will be dealing with a single binary PA, meaning there will only be two PA groups, with every datapoint belonging to one of these groups.
\end{definition}

\begin{definition}[Positive Ratio]
  We will call the ratio of the number of positive instances divided by the total number of instances in a specific group the \emph{positive ratio} (PR) of the group.
\end{definition}

\begin{definition}[Favoured and Unfavoured Groups]
  Among the two PA groups, the one having the highest PR will be referred to as the \emph{favoured} ($F$) group, while the other one will be referred to as the \emph{unfavoured} ($U$) group.
\end{definition}

\subsubsection{Fairness Metrics}

Even though there are many different fairness definitions~\cite{kusner2017counterfactual,kilbertus2017avoiding}, on this paper we'll focus on two of them.

\begin{definition}[Demographic Parity]
  A classifier satisfies \emph{demographic parity} if the probability of being classified as positive is the same across PA subgroups:
  \begin{equation}
    \label{eq:dp}
    \text{P} \left( \hat{Y} = 1 \mid \text{PA} = U \right) =
    \text{P} \left( \hat{Y} = 1 \mid \text{PA} = F \right).
  \end{equation}
\end{definition}

\begin{definition}[Equality of Opportunity]
  A classifier satisfies \emph{equality of opportunity} if the probability of being classified as positive \emph{for true positives} is the same across PA subgroups:
  \begin{equation}
    \label{eq:eo}
    \text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = U \right) =
    \text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = F \right).
  \end{equation}
\end{definition}

Demographic parity and equality of opportunity may be expressed as the following ratios, respectively:
\begin{definition}[Fairness Ratios]
  \label{def:fratios}
  \begin{align}
    \text{DPR} &\coloneqq \frac
    {\text{P} \left( \hat{Y} = 1 \mid \text{PA} = U \right)}
    {\text{P} \left( \hat{Y} = 1 \mid \text{PA} = F \right)}, \\
    \text{EOR} &\coloneqq \frac
    {\text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = U \right)}
    {\text{P} \left( \hat{Y} = 1 \mid Y = 1,\, \text{PA} = F \right)}.
  \end{align}
\end{definition}

These ratios will measure how fair a classifier's predictions are with respect to a fairness definition: the closer the ratio is to 1, the fairer a classifier will be.

\subsubsection{Correcting for Fair Classification}

Fairness-aware machine learning is defined by~\cite{friedler2019comparative} as a set of preprocessing techniques that modify input data so that any classifier trained on such data will be fair. According to~\cite{kamiran2012data}, there are four main ways in which to make appropriate adjustments to data in order to enforce fairness: suppressing certain features, also known as \emph{fairness through unawareness}~\cite{gajane2017formalizing},  reweighing features~\cite{krasanakis2018adaptive}, resampling data instances~\cite{rubin1973use,kamiran2010classification,salimi2019capuchin,zelaya2019parametrised} and \emph{massaging} variable values~\cite{chiappa2018path}; our method belongs to this last category.

\subsection{Privacy Related Work and Definitions}

There is a vast and growing literature on privacy protection techniques. 
Regarding data publishing we may separate them on sampling, aggregating (e.g., $k$-anonymity) and noise addition (e.g., differential privacy) techniques. There are some differences and interactions between them in different contexts, some of them are discussed in \cite{Salas:2018}. However, we will focus here on aggregation techniques.

Aggregation techniques such as $k$-anonymity were defined for tables in \cite{Samarati:2001,Sweeney:2002}, after showing that removing all the identifiers (IDs), such as Social Security Number or Name-surname, does not guarantees that the individuals cannot be re-identified. Unique combinations of attribute values called quasi-identifiers (QIs) may be used instead of the IDs, then, after re-identification, individuals' Sensitive Attributes (SA) are revealed (e.g., Salary, Medical conditions, etc.) 

\begin{definition}[$k$-Anonymity]
A dataset is \emph{$k$-anonymous} if each record is indistinguishable from at least other $k - 1$ records within the dataset, when considering the values of its QIs. 
\end{definition}

There are several techniques for obtaining $k$-anonymous datasets, for example, generalization and suppression or microaggregation.

\begin{definition}[Microaggregation]
To obtain microaggregates in dataset with $n$ records, these are combined to form groups $g$ of size at least $k$ ($k$-groups). For each attribute, the average value over each group is computed and is used to replace each of the original averaged values. Groups are formed using a criterion of maximal similarity.
\end{definition}

The optimal $k$-partition (from the information loss point of view) is defined to be the one that maximizes within-group homogeneity. The higher the within-group homogeneity, the lower the information loss, since microaggregation replaces values in a group by the group centroid. The sum of squares criterion is common to measure homogeneity in clustering. The within-groups sum of squares SSE is defined as:
$$SSE= \displaystyle\sum_{j=1}^{n_g}\sum_{i=1}^{n_j}d(x_{ij},\overline{x}_j)^2$$
Where $x_{ij}$ denotes the $i-$th record in the $j-$th group, $\overline{x}_j$ is the average record in the group $j$, $n_g$ is the number of groups and $n_j$ is the number of elements in the $j-$th group.

MDAV \cite{Domingo:2005} is an algorithm for $k$-anonymisation based on microaggregation it has been used extensively and modified in several different ways. Two recent examples are \cite{Salas:2018-b}, in which it is used for $k$-anonymisation of dynamic data and other is \cite{Rodriguez:2020}, in which MDAVs efficiency is largely improved without losing precision. 

By design $k$-anonymity guarantees that a user's record is protected from linkage by the values of similar users, however, if the SAs are not modified, then a low variability of SAs in a group will help an adversary to infer the SAs of a user, even without being able to link her. 
Hence, some modifications to $k$-anonymity have been done, such as $\ell$-diversity \cite{Machanavajjhala:2006} and $t$-closeness \cite{Li:2007}. 

\begin{definition}[$t$-Closeness]
An equivalence class satisfies $t$-closeness if the distance between the distribution of the SAs of the individuals in the group to the distribution of the SAs in the whole table is no more than a threshold $t$.
A dataset $D$ (usually a $k$-anonymous dataset) satisfies \emph{$t$-closeness} if all its equivalence classes ($k$-groups) satisfy $t$-closeness.
\end{definition}
